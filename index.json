
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":" I am a Research Scientist in the GenAI Research group at Meta. My current research focuses on understanding and generating multimodal data, using minimal human supervision. I obtained a MS and PhD in Robotics from Carnegie Mellon University (here’s a link to my dissertation), where I worked on learning from and understanding videos. I was previously part of the Facebook AI Research (FAIR) group at Meta, and have spent time at DeepMind, Adobe and Facebook as an intern. See here for a formal bio.\nNews [October\u0026#39;2024] Mark Zuckerberg announced our work on MovieGen, the new state-of-the-art media generation and editing system, outperforming SORA, Emu Video and more! Covered in NY Times, FT, Forbes, WIRED, Bloomberg, TechCrunch, etc. [July\u0026#39;2024] Mark Zuckerberg announced Llama 3.1, along with our state-of-the-art video recognition capabilities! [June\u0026#39;2024] Invited panelist for the AI for Content Creation (AI4CC) workshop at CVPR 2024 (along with Cynthia Lu and Robin Rombach). [June\u0026#39;2024] LaViLa and Ego4D among the winners of the EgoVis 2022-23 Distinguished Paper Awards! [April\u0026#39;2024] Presented Emu Video at RunwayML\u0026#39;s inaugural Research and Art (RNA) event. [Feb\u0026#39;2024] Invited judge for the MIT Filmmaking Hackathhon 2024. [April\u0026#39;2024] /animate functionality based on Emu Video is publicly released! Try it out to animate images generated using /imagine on meta.ai! [Nov\u0026#39;2023] Mark Zuckerberg announced our state-of-the-art video generation work, Emu Video! Also see coverage by TechCrunch, TheVerge, VentureBeat, Reuters, and others! [Oct\u0026#39;2023] Giving talks at the DeepMind AI Video symposium, and Perception Test workshop at ICCV 2023 (video). [June\u0026#39;2023] Giving a talk at HVU Workshop and presenting 5 papers at CVPR 2023! [May\u0026#39;2023] Mark Zuckerberg announced our multimodal embedding work, ImageBind! Also see coverage by TheVerge, Engadget, SiliconANGLE, maginative and others! [June\u0026#39;2022] Presenting 3 papers at CVPR 2022, including Omivore, a single model that obtains state-of-the-art results across 3 different modalities: images, videos and single-view 3D! [Oct\u0026#39;2021] We announced Ego4D, the largest egocentric video dataset to date! See this video for a quick intro, and see coverage from TechCrunch, TheVerge, Axios, Fast Company, and others! ","date":1738195200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1738195200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"2025-01-30T00:00:00Z","relpermalink":"","section":"authors","summary":" I am a Research Scientist in the GenAI Research group at Meta. My current research focuses on understanding and generating multimodal data, using minimal human supervision. I obtained a MS and PhD in Robotics from Carnegie Mellon University (here’s a link to my dissertation), where I worked on learning from and understanding videos. I was previously part of the Facebook AI Research (FAIR) group at Meta, and have spent time at DeepMind, Adobe and Facebook as an intern. See here for a formal bio.\n","tags":null,"title":"Rohit Girdhar","type":"authors"},{"authors":["Yinbo Chen","Rohit Girdhar","Xiaolong Wang","Sai Saketh Rambhatla","Ishan Misra"],"categories":null,"content":"Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models. We present a simple diffusion tokenizer (DiTo) that learns compact visual representations for image generation models. Our key insight is that a single learning objective, diffusion L2 loss, can be used for training scalable image tokenizers. Since diffusion is already widely used for image generation, our insight greatly simplifies training such tokenizers. In contrast, current state-of-the-art tokenizers rely on an empirically found combination of heuristics and losses, thus requiring a complex training recipe that relies on non-trivially balancing different losses and pretrained supervised models. We show design decisions, along with theoretical grounding, that enable us to scale DiTo for learning competitive image representations. Our results show that DiTo is a simpler, scalable, and self-supervised alternative to the current state-of-the-art image tokenizer which is supervised. DiTo achieves competitive or better quality than state-of-the-art in image reconstruction and downstream image generation tasks.\n","date":1738195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738195200,"objectID":"1b8ab0953b13a95d3162ee31c185dee2","permalink":"http://localhost:1313/publication/2025_dito/","publishdate":"2025-01-30T00:00:00Z","relpermalink":"/publication/2025_dito/","section":"publication","summary":"Simplified image tokenization using diffusion","tags":["Generative"],"title":"Diffusion Autoencoders are Scalable Image Tokenizers","type":"publication"},{"authors":["Kumar Ashutosh","Yossi Gandelsman","Xinlei Chen","Ishan Misra","Rohit Girdhar"],"categories":null,"content":"We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple, training-free approach, to imbue multimodal capabilities into your favorite LLM. Leveraging their innate ability to perform multi-step reasoning, MILS prompts the LLM to generate candidate outputs, each of which are scored and fed back iteratively, eventually generating a solution to the task. This enables various applications that typically require training specialized models on task-specific data. In particular, we establish a new state-of-the-art on emergent zero-shot image, video and audio captioning. MILS seamlessly applies to media generation as well, discovering prompt rewrites to improve text-to-image generation, and even edit prompts for style transfer! Finally, being a gradient-free optimization approach, MILS can invert multimodal embeddings into text, enabling applications like cross-modal arithmetic.\n","date":1738195200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738195200,"objectID":"293446238514cf3ed41425c45465f2ab","permalink":"http://localhost:1313/publication/2025_mils/","publishdate":"2025-01-30T00:00:00Z","relpermalink":"/publication/2025_mils/","section":"publication","summary":"Pure text-only LLMs can use off-the-shelf multimodal embedding models to do various multimodal tasks!","tags":["Selected","Multimodal","Generative"],"title":"LLMs can see and hear without any training","type":"publication"},{"authors":["Shijie Wang","Samaneh Azadi","Rohit Girdhar","Saketh Rambhatla","Chen Sun","Xi Yin"],"categories":null,"content":"Text-Image-to-Video (TI2V) generation aims to generate a video from an image following a text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, a simple yet effective approach that directs the model’s learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate a motion heatmap and weight the loss according to the intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, a dataset consists of 320 image-text pairs for robust evaluation. We present a human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in this https URL.\n","date":1734652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734652800,"objectID":"e491390b8ec682f133ed4231455ca0c3","permalink":"http://localhost:1313/publication/2024_motif/","publishdate":"2024-12-20T00:00:00Z","relpermalink":"/publication/2024_motif/","section":"publication","summary":"Using flow to improve motion in video generation","tags":["Generative","Video"],"title":"MotiF: Making Text Count in Image Animation with Motion Focal Loss","type":"publication"},{"authors":["MovieGen team (core-contributor)"],"categories":null,"content":"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user’s image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at this https URL.\n","date":1729123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729123200,"objectID":"249e6b4524e32a7d39e79060bca3e79c","permalink":"http://localhost:1313/publication/2024_moviegen/","publishdate":"2024-10-17T00:00:00Z","relpermalink":"/publication/2024_moviegen/","section":"publication","summary":"State-of-the-Art Video (+Audio) Generation Model","tags":["Selected","Multimodal","Generative","Video"],"title":"Movie Gen: A Cast of Media Foundation Models","type":"publication"},{"authors":["Llama3 team (co-lead the video recognition efforts)"],"categories":null,"content":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.\n","date":1721692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721692800,"objectID":"517048bb6735221b941c31ae9f239980","permalink":"http://localhost:1313/publication/2024_llama3/","publishdate":"2024-07-23T00:00:00Z","relpermalink":"/publication/2024_llama3/","section":"publication","summary":"State-of-the-Art open-source LLM with multimodal capabilities","tags":["Selected","Multimodal","Video"],"title":"The Llama 3 Herd of Models","type":"publication"},{"authors":["Changan Chen","Ashutosh Kumar","Rohit Girdhar","David Harwath","Kristen Grauman"],"categories":null,"content":"We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not. We show our approach can successfully discover how the long tail of human actions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks.\n","date":1712534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712534400,"objectID":"9363afe4be070785e72b25f267d6d9c2","permalink":"http://localhost:1313/publication/2024_soundingactions/","publishdate":"2024-04-08T00:00:00Z","relpermalink":"/publication/2024_soundingactions/","section":"publication","summary":"Self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos.","tags":["Multimodal","Video","Representation"],"title":"SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos","type":"publication"},{"authors":["Xudong Wang","Trevor Darrell","Sai Saketh Rambhatla","Rohit Girdhar","Ishan Misra"],"categories":null,"content":"Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP50box for box inputs, and 25.4% IoU for mask inputs.\n","date":1707091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707091200,"objectID":"219094f2b5b65b5a6b46f01afa50c678","permalink":"http://localhost:1313/publication/2024_instancediffusion/","publishdate":"2024-02-05T00:00:00Z","relpermalink":"/publication/2024_instancediffusion/","section":"publication","summary":"SOTA instance-conditioned diffusion model for image generation.","tags":["Spatial","Generative"],"title":"InstanceDiffusion: Instance-level Control for Image Generation","type":"publication"},{"authors":["Sachit Menon","Ishan Misra","Rohit Girdhar"],"categories":null,"content":"We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user’s needs. We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations. We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input. The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles. Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user’s individual situation.\n","date":1701907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701907200,"objectID":"17920c99e36fc5ec8376c7a6d6ab0d37","permalink":"http://localhost:1313/publication/2023_illustratedinstructions/","publishdate":"2023-12-07T00:00:00Z","relpermalink":"/publication/2023_illustratedinstructions/","section":"publication","summary":"Introducing a new task of generating instructions for the task you want to solve with illustrations, and a LLM + Diffusion model based solution.","tags":["Generative","Multimodal"],"title":"Generating Illustrated Instructions","type":"publication"},{"authors":["Wilson Yan","Andrew Brown","Pieter Abbeel","Rohit Girdhar","Samaneh Azadi"],"categories":null,"content":"We introduce MoCA, a Motion-Conditioned Image Animation approach for video editing. It leverages a simple decomposition of the video editing problem into image editing followed by motion-conditioned image animation. Furthermore, given the lack of robust evaluation datasets for video editing, we introduce a new benchmark that measures edit capability across a wide variety of tasks, such as object replacement, background changes, style changes, and motion edits. We present a comprehensive human evaluation of the latest video editing methods along with MoCA, on our proposed benchmark. MoCA establishes a new state-of-the-art, demonstrating greater human preference win-rate, and outperforming notable recent approaches including Dreamix (63%), MasaCtrl (75%), and Tune-A-Video (72%), with especially significant improvements for motion edits.\n","date":1701302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701302400,"objectID":"97cbcbe4afe52f50da353984a56194ec","permalink":"http://localhost:1313/publication/2023_moca/","publishdate":"2023-11-30T00:00:00Z","relpermalink":"/publication/2023_moca/","section":"publication","summary":"Image animation FTW again! SOTA video editing results by animating the first frame with motion conditioning.","tags":["Generative","Multimodal","Video"],"title":"Motion-Conditioned Image Animation for Video Editing","type":"publication"},{"authors":["Xudong Wang","Ishan Misra","Ziyun Zeng","Rohit Girdhar","Trevor Darrell"],"categories":null,"content":"Existing approaches to unsupervised video instance segmentation typically rely on motion estimates and experience difficulties tracking small or divergent motions. We present VideoCutLER, a simple method for unsupervised multi-instance video segmentation without using motion-based learning signals like optical flow or training on natural videos. Our key insight is that using high-quality pseudo masks and a simple video synthesis method for model training is surprisingly sufficient to enable the resulting video model to effectively segment and track multiple instances across video frames. We show the first competitive unsupervised learning results on the challenging YouTubeVIS-2019 benchmark, achieving 50.7% APvideo^50 , surpassing the previous state-of-the-art by a large margin. VideoCutLER can also serve as a strong pretrained model for supervised video instance segmentation tasks, exceeding DINO by 15.9% on YouTubeVIS-2019 in terms of APvideo.\n","date":1701302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701302400,"objectID":"375432b99ae9c9a2986737b1ad7640aa","permalink":"http://localhost:1313/publication/2023_videocutler/","publishdate":"2023-11-30T00:00:00Z","relpermalink":"/publication/2023_videocutler/","section":"publication","summary":"SOTA unsupervised video segmentation using CutLER.","tags":["Spatial","Representation","Video"],"title":"VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation","type":"publication"},{"authors":["Rohit Girdhar","Mannat Singh","Andrew Brown","Quentin Duval","Samaneh Azadi","Sai Saketh Rambhatla","Akbar Shah","Xi Yin","Devi Parikh","Ishan Misra"],"categories":null,"content":"We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions–adjusted noise schedules for diffusion, and multi-stage training–that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work–81% vs. Google’s Imagen Video, 90% vs. Nvidia’s PYOCO, and 96% vs. Meta’s Make-A-Video. Our model outperforms commercial solutions such as RunwayML’s Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user’s text prompt, where our generations are preferred 96% over prior work.\n","date":1700006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700006400,"objectID":"6457dd6f32dd63dccb65859589177b8e","permalink":"http://localhost:1313/publication/2023_emuvideo/","publishdate":"2023-11-15T00:00:00Z","relpermalink":"/publication/2023_emuvideo/","section":"publication","summary":"A simple and effective approach to high-quality video generation by learning to animate high quality images.","tags":["Selected","Generative","Multimodal","Video"],"title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning","type":"publication"},{"authors":["Rohit Girdhar","Alaaeldin El-Nouby","Zhuang Liu","Mannat Singh","Kalyan Vasudev Alwala","Armand Joulin","Ishan Misra"],"categories":null,"content":"We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications ‘out-of-the-box’ including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.\n","date":1683590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683590400,"objectID":"4c673cc26fbf7efa6c8b3fd2df0f2c98","permalink":"http://localhost:1313/publication/2023_imagebind/","publishdate":"2023-05-09T00:00:00Z","relpermalink":"/publication/2023_imagebind/","section":"publication","summary":"One embedding space for 6 different modalities, enables zero-shot recognition on all modalities!","tags":["Selected","Multimodal","Video"],"title":"ImageBind: One Embedding Space To Bind Them All","type":"publication"},{"authors":["Mannat Singh","Quentin Duval","Kalyan Vasudev Alwala","Haoqi Fan","Vaibhav Aggarwal","Aaron Adcock","Armand Joulin","Piotr Dollár","Christoph Feichtenhofer","Ross Girshick","Rohit Girdhar","Ishan Misra"],"categories":null,"content":"This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images.\n","date":1679529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679529600,"objectID":"5576fc789a01abf41bd76b6bc212d13c","permalink":"http://localhost:1313/publication/2023_maws/","publishdate":"2023-03-23T00:00:00Z","relpermalink":"/publication/2023_maws/","section":"publication","summary":"Scaling up MAE pre-pretraining, followed by weakly supervised pretraining, leads to strong representations.","tags":["Representation","Video"],"title":"The effectiveness of MAE pre-pretraining for billion-scale pretraining","type":"publication"},{"authors":["Xudong Wang","Rohit Girdhar","Stella X. Yu","Ishan Misra"],"categories":null,"content":"We propose Cut-and-LEaRn (CutLER) which is a simple approach for training unsupervised object detection and segmentation models. We leverage the property of selfsupervised models to “discover” objects without supervision and amplify it to train a state-of-the-art localization model without any human labels. CutLER first uses our proposed MaskCut approach to generate coarse masks for multiple objects in an image, and then learns a detector on these masks using our robust loss function. We further improve performance by self-training the model on its predictions. Compared to prior work, CutLER is simpler, compatible with different detection architectures, and detects multiple objects. CutLER is also a zero-shot unsupervised detector and improves detection performance by over 2.7 times on 11 benchmarks across domains like video frames, paintings, sketches, etc. With finetuning, CutLER serves as a lowshot detector surpassing MoCo-v2 by 7.3% APbox and 6.5% APmask on COCO when training with 5% labels.\n","date":1674691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674691200,"objectID":"3a3a77ac89c9fbebd274b696260c124a","permalink":"http://localhost:1313/publication/2023_cutler/","publishdate":"2023-01-26T00:00:00Z","relpermalink":"/publication/2023_cutler/","section":"publication","summary":"Discovering objects using DINO features, and learning an unsupervised detection + segmentation model","tags":["Spatial","Representation"],"title":"CutLER: Cut and Learn for Unsupervised Object Detection and Instance Segmentation","type":"publication"},{"authors":["Kumar Ashutosh","Rohit Girdhar","Lorenzo Torresani","Kristen Grauman"],"categories":null,"content":"Video-language embeddings are a promising avenue for injecting semantics into visual representations, but existing methods capture only short-term associations between seconds-long video clips and their accompanying text. We propose HierVL, a novel hierarchical video-language embedding that simultaneously accounts for both long-term and short-term associations. As training data, we take videos accompanied by timestamped text descriptions of human actions, together with a high-level text summary of the activity throughout the long video (as are available in Ego4D). We introduce a hierarchical contrastive training objective that encourages text-visual alignment at both the clip level and video level. While the clip-level constraints use the step-by-step descriptions to capture what is happening in that instant, the video-level constraints use the summary text to capture why it is happening, i.e., the broader context for the activity and the intent of the actor. Our hierarchical scheme yields a clip representation that outperforms its single-level counterpart as well as a long-term video representation that achieves SotA results on tasks requiring long-term video modeling. HierVL successfully transfers to multiple challenging downstream tasks (in EPIC-KITCHENS-100, Charades-Ego, HowTo100M) in both zero-shot and fine-tuned settings.\n","date":1672876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672876800,"objectID":"d6e3f20c0001f530f865049282c5d0eb","permalink":"http://localhost:1313/publication/2023_hiervl/","publishdate":"2023-01-05T00:00:00Z","relpermalink":"/publication/2023_hiervl/","section":"publication","summary":"Video-language embeddings are a promising avenue for injecting semantics into visual representations, but existing methods capture only short-term associations between seconds-long video clips and their accompanying text. We propose HierVL, a novel hierarchical video-language embedding that simultaneously accounts for both long-term and short-term associations. As training data, we take videos accompanied by timestamped text descriptions of human actions, together with a high-level text summary of the activity throughout the long video (as are available in Ego4D). We introduce a hierarchical contrastive training objective that encourages text-visual alignment at both the clip level and video level. While the clip-level constraints use the step-by-step descriptions to capture what is happening in that instant, the video-level constraints use the summary text to capture why it is happening, i.e., the broader context for the activity and the intent of the actor. Our hierarchical scheme yields a clip representation that outperforms its single-level counterpart as well as a long-term video representation that achieves SotA results on tasks requiring long-term video modeling. HierVL successfully transfers to multiple challenging downstream tasks (in EPIC-KITCHENS-100, Charades-Ego, HowTo100M) in both zero-shot and fine-tuned settings.\n","tags":["Multimodal","Video"],"title":"HierVL: Learning Hierarchical Video-Language Embeddings","type":"publication"},{"authors":["Yue Zhao","Ishan Misra","Philipp Krähenbühl","Rohit Girdhar"],"categories":null,"content":"Video-language embeddings are a promising avenue for injecting semantics into visual representations, but existing methods capture only short-term associations between seconds-long video clips and their accompanying text. We propose HierVL, a novel hierarchical video-language embedding that simultaneously accounts for both long-term and short-term associations. As training data, we take videos accompanied by timestamped text descriptions of human actions, together with a high-level text summary of the activity throughout the long video (as are available in Ego4D). We introduce a hierarchical contrastive training objective that encourages text-visual alignment at both the clip level and video level. While the clip-level constraints use the step-by-step descriptions to capture what is happening in that instant, the video-level constraints use the summary text to capture why it is happening, i.e., the broader context for the activity and the intent of the actor. Our hierarchical scheme yields a clip representation that outperforms its single-level counterpart as well as a long-term video representation that achieves SotA results on tasks requiring long-term video modeling. HierVL successfully transfers to multiple challenging downstream tasks (in EPIC-KITCHENS-100, Charades-Ego, HowTo100M) in both zero-shot and fine-tuned settings.\n","date":1670457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670457600,"objectID":"079aecc19a9fa063327e2206051b3265","permalink":"http://localhost:1313/publication/2022_lavila/","publishdate":"2022-12-08T00:00:00Z","relpermalink":"/publication/2022_lavila/","section":"publication","summary":"Leveraging LLMs to auto-annotate videos for representation learning.","tags":["Selected","Multimodal","Video"],"title":"Learning Video Representations from Large Language Models","type":"publication"},{"authors":["Rohit Girdhar","Alaaeldin El-Nouby","Mannat Singh","Kalyan Vasudev Alwala","Armand Joulin","Ishan Misra"],"categories":null,"content":"Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Huge model can be finetuned to achieve 86.6% on ImageNet and 75.5% on the challenging Something Something-v2 video benchmark, setting a new state-of-the-art.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"dd7579aa16ce2fc00dee956a216c1a63","permalink":"http://localhost:1313/publication/2022_omnimae/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/publication/2022_omnimae/","section":"publication","summary":"Single self-supervised representation for images and videos.","tags":["Representation","Multimodal","Video"],"title":"OmniMAE: Single Model Masked Pretraining on Images and Videos","type":"publication"},{"authors":["Rohit Girdhar","Mannat Singh","Nikhila Ravi","Laurens van der Maaten","Armand Joulin","Ishan Misra"],"categories":null,"content":"Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our ‘OMNIVORE’ model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. OMNIVORE is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modality-specific models of the same size. A single OMNIVORE model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. OMNIVORE’s shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.\n","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"6ec3faa9925043d3ba55829c42c25fec","permalink":"http://localhost:1313/publication/2022_omnivore/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/publication/2022_omnivore/","section":"publication","summary":"A single model for images, video and single-view 3D.","tags":["Selected","Representation","Multimodal","Video"],"title":"Omnivore: A Single Model for Many Visual Modalities","type":"publication"},{"authors":["Kristen Grauman","Andrew Westbury","Rohit Girdhar","et al"],"categories":null,"content":"We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception.\n","date":1648598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648598400,"objectID":"de81660e3a162705a5c339b88bf5e12a","permalink":"http://localhost:1313/publication/2022_ego4d/","publishdate":"2022-03-30T00:00:00Z","relpermalink":"/publication/2022_ego4d/","section":"publication","summary":"The largest egocentric video dataset.","tags":["Selected","Multimodal","Video"],"title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video","type":"publication"},{"authors":["Xingyi Zhou","Rohit Girdhar","Armand Joulin","Philipp Krähenbühl","Ishan Misra"],"categories":null,"content":"Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning.\n","date":1641513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641513600,"objectID":"5e721871b878a1ab47f7d089ff7715fd","permalink":"http://localhost:1313/publication/2022_detic/","publishdate":"2022-01-07T00:00:00Z","relpermalink":"/publication/2022_detic/","section":"publication","summary":"Leverages image classification data to build an object detector","tags":["Spatial","Representation"],"title":"Detecting Twenty-thousand Classes using Image-level Supervision","type":"publication"},{"authors":["Bowen Cheng","Anwesa Choudhuri","Ishan Misra","Alexander Kirillov","Rohit Girdhar","Alexander G. Schwing"],"categories":null,"content":"We find Mask2Former also achieves state-of-the-art performance on video instance segmentation without modifying the architecture, the loss or even the training pipeline. In this report, we show universal image segmentation architectures trivially generalize to video segmentation by directly predicting 3D segmentation volumes. Specifically, Mask2Former sets a new state-of-the-art of 60.4 AP on YouTubeVIS-2019 and 52.6 AP on YouTubeVIS-2021. We believe Mask2Former is also capable of handling video semantic and panoptic segmentation, given its versatility in image segmentation. We hope this will make state-of-the-art video segmentation research more accessible and bring more attention to designing universal image and video segmentation architectures.\n","date":1639958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639958400,"objectID":"0b607f2dbc52fd5eba0a100623cca895","permalink":"http://localhost:1313/publication/2021_videomask2former/","publishdate":"2021-12-20T00:00:00Z","relpermalink":"/publication/2021_videomask2former/","section":"publication","summary":"SOTA video segmentation using Mask2Former.","tags":["Video","Spatial"],"title":"Mask2Former for Video Instance Segmentation","type":"publication"},{"authors":["Bowen Cheng","Ishan Misra","Alexander G. Schwing","Alexander Kirillov","Rohit Girdhar"],"categories":null,"content":"Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).\n","date":1638403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638403200,"objectID":"9a46f2a1b5069f23cd68500437e42013","permalink":"http://localhost:1313/publication/2021_mask2former/","publishdate":"2021-12-02T00:00:00Z","relpermalink":"/publication/2021_mask2former/","section":"publication","summary":"Single architecture state-of-the-art in instance, semantic and panoptic segmentation.","tags":["Selected","Spatial"],"title":"Masked-attention Mask Transformer for Universal Image Segmentation","type":"publication"},{"authors":["Ishan Misra","Rohit Girdhar","Armand Joulin"],"categories":null,"content":"We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3D specific inductive biases, 3DETR requires minimal modifications to the vanilla Transformer block. Specifically, we find that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3D specific operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.\n","date":1631750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631750400,"objectID":"8e7727f331aca84ec1d1962f13e91778","permalink":"http://localhost:1313/publication/2021_3detr/","publishdate":"2021-09-16T00:00:00Z","relpermalink":"/publication/2021_3detr/","section":"publication","summary":"First Transformer based detection architecture for 3D data.","tags":["ThreeD","Spatial"],"title":"3DETR: An End-to-End Transformer Model for 3D Object Detection","type":"publication"},{"authors":["Rohit Girdhar","Kristen Grauman"],"categories":null,"content":"We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to predict the next action in a video sequence, while also learning frame feature encoders that are predictive of successive future frames’ features. Compared to existing temporal aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies–both critical for the anticipation task. Through extensive experiments, we show that AVT obtains the best reported performance on four popular action anticipation benchmarks: EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins first place in the EpicKitchens-100 CVPR\u0026#39;21 challenge.\n","date":1622678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622678400,"objectID":"443e65804dcf522bcf23c27361dac817","permalink":"http://localhost:1313/publication/2021_avt/","publishdate":"2021-06-03T00:00:00Z","relpermalink":"/publication/2021_avt/","section":"publication","summary":"An autoregressive video transformer architecture for action anticipation in videos.","tags":["Selected","Video"],"title":"Anticipative Video Transformer","type":"publication"},{"authors":["Zhongzheng Ren","Ishan Misra","Alexander G. Schwing","Rohit Girdhar"],"categories":null,"content":"We introduce WyPR, a Weakly-supervised framework for Point cloud Recognition, requiring only scene-level class tags as supervision. WyPR jointly addresses three core 3D recognition tasks: point-level semantic segmentation, 3D proposal generation, and 3D object detection, coupling their predictions through self and cross-task consistency losses. We show that in conjunction with standard multiple-instance learning objectives, WyPR can detect and segment objects in point cloud without access to any spatial labels at training time. We demonstrate its efficacy using the ScanNet and S3DIS datasets, outperforming prior state of the art on weakly-supervised segmentation by more than 6% mIoU. In addition, we set up the first benchmark for weakly-supervised 3D object detection on both datasets, where WyPR outperforms standard approaches and establishes strong baselines for future work.\n","date":1622419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622419200,"objectID":"09d17066ca3afc3435940be1be1a7c08","permalink":"http://localhost:1313/publication/2021_wypr/","publishdate":"2021-05-31T00:00:00Z","relpermalink":"/publication/2021_wypr/","section":"publication","summary":"WyPR can detect and segment objects in a 3D scene without needing any spatial labels at all!","tags":["ThreeD","Spatial"],"title":"3D Spatial Recognition without Spatially Labeled 3D","type":"publication"},{"authors":["Zaiwei Zhang","Rohit Girdhar","Armand Joulin","Ishan Misra"],"categories":null,"content":"Pretraining on large labeled datasets is a prerequisite to achieve good performance in many computer vision tasks like 2D object recognition, video classification etc. However, pretraining is not widely used for 3D recognition tasks where state-of-the-art methods train models from scratch. A primary reason is the lack of large annotated datasets because 3D data is both difficult to acquire and time consuming to label. We present a simple self-supervised pertaining method that can work with any 3D data - single or multiview, indoor or outdoor, acquired by varied sensors, without 3D registration. We pretrain standard point cloud and voxel based model architectures, and show that joint pretraining further improves performance. We evaluate our models on 9 benchmarks for object detection, semantic segmentation, and object classification, where they achieve state-of-the-art results and can outperform supervised pretraining. We set a new state-of-the-art for object detection on ScanNet (69.0% mAP) and SUNRGBD (63.5% mAP). Our pretrained models are label efficient and improve performance for classes with few examples.\n","date":1622419200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622419200,"objectID":"d6be39106b33cffc4539820b5b15a46e","permalink":"http://localhost:1313/publication/2021_depthcontrast/","publishdate":"2021-05-31T00:00:00Z","relpermalink":"/publication/2021_depthcontrast/","section":"publication","summary":"SOTA 3D detection/segmentation results by learning contrastive representations on 3D data","tags":["ThreeD","Representation"],"title":"Self-Supervised Pretraining of 3D Features on any Point-Cloud","type":"publication"},{"authors":["Eltayeb Ahmed","Anton Bakhtin","Laurens van der Maaten","Rohit Girdhar"],"categories":null,"content":"A common approach to solving physicalreasoning tasks is to train a value learner on example tasks. A limitation of such an approach is that it requires learning about object dynamics solely from reward values assigned to the final state of a rollout of the environment. This study aims to address this limitation by augmenting the reward value with self-supervised signals about object dynamics. Specifically, we train the model to characterize the similarity of two environment rollouts, jointly with predicting the outcome of the reasoning task. This similarity can be defined as a distance measure between the trajectory of objects in the two rollouts, or learned directly from pixels using a contrastive formulation. Empirically, we find that this approach leads to substantial performance improvements on the PHYRE benchmark for physical reasoning, establishing a new state-of-the-art.\n","date":1613779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613779200,"objectID":"d8b9fbabb174ba5c99d13c50f5672bb4","permalink":"http://localhost:1313/publication/2021_dynamicsaware/","publishdate":"2021-02-20T00:00:00Z","relpermalink":"/publication/2021_dynamicsaware/","section":"publication","summary":"Self-supervised representations for physical reasoning.","tags":["Physics","Video"],"title":"Physical Reasoning Using Dynamics Aware Embeddings","type":"publication"},{"authors":["Rohit Girdhar","Laura Gustafson","Aaron Adcock","Laurens van der Maaten"],"categories":null,"content":"Physical reasoning requires forward prediction: the ability to forecast what will happen next given some initial world state. We study the performance of state-of-the-art forward-prediction models in the complex physical-reasoning tasks of the PHYRE benchmark. We do so by incorporating models that operate on object or pixel-based representations of the world into simple physical-reasoning agents. We find that forward-prediction models can improve physical-reasoning performance, particularly on complex tasks that involve many objects. However, we also find that these improvements are contingent on the test tasks being small variations of train tasks, and that generalization to completely new task templates is challenging. Surprisingly, we observe that forward predictors with better pixel accuracy do not necessarily lead to better physical-reasoning performance.Nevertheless, our best models set a new state-of-the-art on the PHYRE benchmark.\n","date":1592438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592438400,"objectID":"adb161e5086cfb879edcfaaebf56629f","permalink":"http://localhost:1313/publication/2020_fwdpred/","publishdate":"2020-06-18T00:00:00Z","relpermalink":"/publication/2020_fwdpred/","section":"publication","summary":"Forward prediction for PHYRE benchmark.","tags":["Physics","Video"],"title":"Forward Prediction for Physical Reasoning","type":"publication"},{"authors":["Rohit Girdhar","Deva Ramanan"],"categories":null,"content":"Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. In this work, we build a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding in order to be solved. Our dataset, named CATER, is rendered synthetically using a library of standard 3D objects, and tests the ability to recognize compositions of object movements that require long-term reasoning. As an illustrative example, consider the “cups and ball” shell game, where a target object (ball) is adversarially passed between container objects (cups) so as to deceive an observer as to its final position. Solving this task requires reasoning about recursive container relations and long-term occlusions. In addition to being a challenging dataset, CATER also provides a plethora of diagnostic tools to analyze modern spatiotemporal video architectures by being completely observable and controllable. Using CATER, we provide insights into some of the most recent state of the art deep video architectures.\n","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"81fcc66db925350bd383d2ccd5b33cbb","permalink":"http://localhost:1313/publication/2019_cater/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/publication/2019_cater/","section":"publication","summary":"A dataset to evaluate temporal reasoning in video models.","tags":["Selected","Video"],"title":"CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning","type":"publication"},{"authors":["Jessica Lee","Deva Ramanan","Rohit Girdhar"],"categories":null,"content":"We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt - or personalize - a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization. One significant benefit of meta-learning is that the personalized transcoder naturally enforces temporal coherence across its generated frames; all frames contain consistent clothing and background geometry of the target. We experiment on in-the-wild internet videos and images and show our approach improves over widely-used baselines for the task.\n","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"0b1f728e82d21bcd3dfb4302e3bb48a3","permalink":"http://localhost:1313/publication/2019_metapix/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/publication/2019_metapix/","section":"publication","summary":"A dataset to evaluate temporal reasoning in video models.","tags":["Generative","Video"],"title":"MetaPix: Few-Shot Video Retargeting","type":"publication"},{"authors":["Rohit Girdhar","Du Tran","Lorenzo Torresani","Deva Ramanan"],"categories":null,"content":"Video recognition models have progressed significantly over the past few years, evolving from shallow classifiers trained on hand-crafted features to deep spatiotemporal networks. However, labeled video data required to train such models have not been able to keep up with the ever-increasing depth and sophistication of these networks. In this work, we propose an alternative approach to learning video representations that require no semantically labeled videos and instead leverages the years of effort in collecting and labeling large and clean still-image datasets. We do so by using state-of-the-art models pre-trained on image datasets as “teachers” to train video models in a distillation framework. We demonstrate that our method learns truly spatiotemporal features, despite being trained only using supervision from still-image networks. Moreover, it learns good representations across different input modalities, using completely uncurated raw video data sources and with different 2D teacher models. Our method obtains strong transfer performance, outperforming standard techniques for bootstrapping video architectures with image-based models by 16%. We believe that our approach opens up new approaches for learning spatiotemporal representations from unlabeled video data.\n","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"f59e007da0a3582d63a6558fb5650add","permalink":"http://localhost:1313/publication/2019_distinit/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/publication/2019_distinit/","section":"publication","summary":"Distilling representations from image models to video models.","tags":["Video","Representation"],"title":"DistInit: Learning Video Representations Without a Single Labeled Video","type":"publication"},{"authors":["Rohit Girdhar","João Carreira","Carl Doersch","Andrew Zisserman"],"categories":null,"content":"We introduce the Action Transformer model for recognizing and localizing human actions in video clips. We repurpose a Transformer-style architecture to aggregate features from the spatiotemporal context around the person whose actions we are trying to classify. We show that by using high-resolution, person-specific, class-agnostic queries, the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others. Additionally its attention mechanism learns to emphasize hands and faces, which are often crucial to discriminate an action - all without explicit supervision other than boxes and class labels. We train and test our Action Transformer network on the Atomic Visual Actions (AVA) dataset, outperforming the state-of-the-art by a significant margin using only raw RGB frames as input.\n","date":1544054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544054400,"objectID":"a7b8342f8a6049d43033e765cf366ec2","permalink":"http://localhost:1313/publication/2018_actiontx/","publishdate":"2018-12-06T00:00:00Z","relpermalink":"/publication/2018_actiontx/","section":"publication","summary":"Among the first applications of Transformers to model videos. SOTA results: close 2nd at AVA Challenge, CVPR'18.","tags":["Selected","Spatial","Video"],"title":"Video Action Transformer Network","type":"publication"},{"authors":["Rohit Girdhar","Georgia Gkioxari","Lorenzo Torresani","Manohar Paluri","Du Tran"],"categories":null,"content":"This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge.\n","date":1514246400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514246400,"objectID":"e4d2c11fc97195a0793427243664a975","permalink":"http://localhost:1313/publication/2018_detectandtrack/","publishdate":"2017-12-26T00:00:00Z","relpermalink":"/publication/2018_detectandtrack/","section":"publication","summary":"Human keypoint tracking approach that ranked first in ICCV 2017 PoseTrack keypoint tracking challenge!","tags":["Selected","Spatial","Video"],"title":"Detect-and-Track: Efficient Pose Estimation in Videos","type":"publication"},{"authors":["Rohit Girdhar","Deva Ramanan"],"categories":null,"content":"We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII dataset with 12.5% relative improvement. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.\n","date":1509753600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509753600,"objectID":"daabc5417cbc4e35eabc58026d522225","permalink":"http://localhost:1313/publication/2017_attentionalpooling/","publishdate":"2017-11-04T00:00:00Z","relpermalink":"/publication/2017_attentionalpooling/","section":"publication","summary":"Among the first applications of attention for contemporary video/action understanding.","tags":["Video"],"title":"Attentional Pooling for Action Recognition","type":"publication"},{"authors":["Rohit Girdhar","Deva Ramanan","Abhinav Gupta","Josef Sivic","Bryan Russell"],"categories":null,"content":"In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks with learnable spatio-temporal NetVLAD codebooks. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13% relative) as well as outperforms other baselines with comparable base architectures on HMDB51, UCF101 and Charades video classification benchmarks.\n","date":1491782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491782400,"objectID":"4a973b450ede6620eaba5a218a927571","permalink":"http://localhost:1313/publication/2017_actionvlad/","publishdate":"2017-04-10T00:00:00Z","relpermalink":"/publication/2017_actionvlad/","section":"publication","summary":"Aggregating visual features for action recognition.","tags":["Video"],"title":"ActionVLAD: Learning spatio-temporal aggregation for action classification","type":"publication"},{"authors":["Xiaolong Wang","Rohit Girdhar","Abhinav Gupta"],"categories":null,"content":"In recent years, there has been a renewed interest in jointly modeling perception and action. At the core of this investigation is the idea of modeling affordances(Affordances are opportunities of interaction in the scene. In other words, it represents what actions can the object be used for). However, when it comes to predicting affordances, even the state of the art approaches still do not use any ConvNets. Why is that? Unlike semantic or 3D tasks, there still does not exist any large-scale dataset for affordances. In this paper, we tackle the challenge of creating one of the biggest dataset for learning affordances. We use seven sitcoms to extract a diverse set of scenes and how actors interact with different objects in the scenes. Our dataset consists of more than 10K scenes and 28K ways humans can interact with these 10K images. We also propose a two-step approach to predict affordances in a new scene. In the first step, given a location in the scene we classify which of the 30 pose classes is the likely affordance pose. Given the pose class and the scene, we then use a Variational Autoencoder (VAE) to extract the scale and deformation of the pose. The VAE allows us to sample the distribution of possible poses at test time. Finally, we show the importance of large-scale data in learning a generalizable and robust model of affordances.\n","date":1459209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459209600,"objectID":"aa9342d6ceb0beed3a2fa1deba57bc05","permalink":"http://localhost:1313/publication/2017_bingewatching/","publishdate":"2016-03-29T00:00:00Z","relpermalink":"/publication/2017_bingewatching/","section":"publication","summary":"Learning how humans interact with their environment by watching TV.","tags":["Video"],"title":"Binge Watching: Scaling Affordance Learning from Sitcoms","type":"publication"},{"authors":["Rohit Girdhar","David F. Fouhey","Mikel Rodriguez","Abhinav Gupta"],"categories":null,"content":"What is a good vector representation of an object? We believe that it should be generative in 3D, in the sense that it can produce new 3D objects; as well as be predictable from 2D, in the sense that it can be perceived from 2D images. We propose a novel architecture, called the TL-embedding network, to learn an embedding space with these properties. The network consists of two components: (a) an autoencoder that ensures the representation is generative; and (b) a convolutional network that ensures the representation is predictable. This enables tackling a number of tasks including voxel prediction from 2D images and 3D model retrieval. Extensive experimental analysis demonstrates the usefulness and versatility of this embedding.\n","date":1459209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459209600,"objectID":"778f253b8c65ca2726278d4248aaf34c","permalink":"http://localhost:1313/publication/2016_tl/","publishdate":"2016-03-29T00:00:00Z","relpermalink":"/publication/2016_tl/","section":"publication","summary":"A single embedding space, good for both generating and understanding 3D models","tags":["Selected","ThreeD","Generative"],"title":"Learning a Predictable and Generative Vector Representation for Objects","type":"publication"}]